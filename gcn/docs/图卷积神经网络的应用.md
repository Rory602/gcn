### 图卷积神经网络的应用

#### 1.简介

​		卷积神经网络提供了在大规模和高维度数据高效地提取有价值的统计特征。CNN具备学习静态的局部结构特征，通过多次的计算，已经在图像、视觉、语音识别方向获得重大突破。CNN提取输入数据局部的属性或者局部特征释放的信号，利用卷积核在这些数据上的学习能力，能够识别出相似的特征。卷积核的转移和不变性意味着能够独立地在空间域识别输入的特征，其尺寸远小于输入数据的维度。

​		社交数据、电信数据、日志数据或者文本数据都是一些重要的、不规则的、非欧式空间领域数据。这些数据可以通过知识图谱表示，它是一种异质实体关系的表示方式。图谱能够编码复杂的拓扑关系。通常的卷积神经网络不能直接利用卷积层和池化层对图谱网络进行操作，这在理论和实践上都存在一定的挑战性。本方法目的就是通过定义一个高效的图谱过滤器，解决CNN在图谱应用上的局限性问题。

#### 2. 思想的产生过程

#####  2.1 图傅里叶的转换

（1）图结构的定义：$\mathcal{G}=(\mathcal{V},\mathcal{E},W)$,  $\mathcal{V}$是有限节点的集合，$|\mathcal{V}|=n$；$\mathcal{E}$是边的集合。$W \in \mathbb{R}^{n \times n}$是邻接权重矩阵，用于描述两点之间的权重。

（2）节点特征：$x_i$第$i$个节点的特征向量，$x \in \mathbb{R}^{n\times n}$。

（3）常用图分析的空间算子-拉普拉斯矩阵$L=D-W$,其中，$W\in \mathbb{R}^{n\times n}$;$D\in \mathbb{R}^{n\times n}$是图的度矩阵，$D_{ii}=\sum_{j}W_{ij}$。

（4）对拉普拉斯矩阵标准化：$L=I_{n}-D^{-1/2}WD^{-1/2}$,其中$I_{n}$是单位矩阵。由于$L$是实对称矩阵，存在非负特征值${\lambda}^{n-1}_{l=0}$，对应的正交特征向量${u_l}_{l=0}^{n-1}$,拉普拉斯矩阵可以利用傅里叶基$U=[u_0,...,u_{n-1}]\in \mathbb{R}^{n\times n}$及其特征值的对角矩阵$\Lambda=\operatorname{diag}([\lambda_0,...,\lambda_{n-1}])\in \mathbb{R}^{n\times n}$表示为$L=U\Lambda U^T$。

（5）利用傅里叶基对节点特征值进行转换$\hat x=U^Tx\in \mathbb{R}^{n\times n}$,这个转化构成过滤器的基础算子。

##### 2.2 频域卷积

$$
g_{\theta}*x=Ug_{\theta}U^Tx\qquad(2-1)
$$

其中$g_{\theta}$可以理解为$L$的特征值的函数，例如：$g_{\theta}(\Lambda)$.

##### 2.3 Chebyshev分解

由于公式2-1的特征向量相乘计算复杂度较高$\mathcal{O}(|N^2|)$,除此之外，特征值的计算也非常消耗资源，采用Chebyshev多项式解决这个问题：
$$
g_{\theta^{\prime}}(\Lambda)\approx \sum_{k=0}^K\theta^{\prime}T_k(\tilde{\Lambda})\qquad (2-3)
$$
其中$\tilde{\Lambda}=2\frac{2}{\lambda_{max}}\Lambda-I_N$其中，$\lambda_{max}$为$L$最大的特征值。$\theta^{\prime}$为Chebyshev的系数。K为距离中心节点的最大步数
$$
T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)\qquad 其中，T_0(x)=1，T_1{x}=x\qquad (2-4)
$$
利用以上公式的定义：特征$x$和过滤器$g_{\theta^{\prime}}$的卷积算子定义：
$$
g_{\theta^{\prime}}\star x\approx \sum_{k=0}^K\theta_k^{\prime}T_k(\tilde{L})x\qquad \tilde{L}=\frac{2}{\lambda_{max}}L-I_N\qquad (2-5)
$$

##### 2.4 GCN

可以通过叠加方程2-5形式的多个卷积层来建立基于图卷积的神经网络模型，每个卷积层非线性。现在，假设我们将逐层卷积运算限制为$K=1$，即图的拉普拉斯谱上的线性函数。这样，我们仍然可以通过叠加多个这样的层来恢复一类丰富的卷积滤波器函数，但我们不限于Chebyshev多项式等给出的显式参数化。我们直观地期望这样一个模型能够缓解节点度分布很广的图（如社会网络、引文网络、知识图和许多其他真实的图数据集）在局部邻域结构上的过度拟合问题。此外，对于固定的计算资源下，这种分层线性公式允许我们建立更深层次的模型，这一做法已知可提高多个领域的建模能力。

在GCN的线性公式中，我们进一步近似于$\lambda_{max}\approx 2$，因为我们可以预期神经网络参数将适应训练中的这种规模变化。在这些近似下，方程2-5简化为：
$$
g_{\theta\prime}\star x\approx\theta_0^{\prime}x+\theta_1^{\prime}\left(L-I_N\right)x=\theta_0^{\prime}x-\theta_1^{\prime}D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x\qquad(2-6)
$$
有两个自由参数$\theta_0^{\prime}$和$\theta^{\prime}_1$。滤波器参数可以在整个图上共享，然后这种形式的滤波器的连续应用有效地卷积节点的第k阶邻域，其中k是神经网络模型中连续滤波操作或卷积层的数目。

在实践中，进一步限制参数的数量有助于解决过度拟合问题，并使每层操作（如矩阵乘法）的数量最小化。这就留给我们以下表达式：
$$
g_{\theta} \star x \approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x \qquad (2-7)
$$
利用单个参数$\theta=\theta_0^{\prime}=-\theta_1^{\prime}$。请注意，在$I_N+D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$的特征值在[0，2]范围内。因此，在神经网络模型中重复使用该算子会导致数值不稳定性和梯度爆炸或消失。为了解决这个问题，应用重新标准化的技巧：$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$,其中，$\tilde{A}=A+I_N$和$\tilde{D}_{ii}=\sum_j\tilde{A}_{ij}$

针对特征$X\in \mathbb{R}^{N\times C}$和过滤器$F$重新定义通用公式，如下：
$$
Z=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta
$$
其中$\Theta \in \mathbb{R}^{C\times F}$是过滤器的参数，$C$是每个节点特征向量的维度，$Z$表示卷积特征矩阵算子$Z\in\mathbb{R}^{N\times F}$

