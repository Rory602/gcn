### 卷积

#### 1、基础概念

由维基百科的介绍我们可以得知，**卷积**是一种定义在两个函数($f$跟$g$)上的数学操作，旨在产生一个新的函数。那么$f$和$g$的卷积就可以写成$f*g$，数学定义如下：
$$
(f*g)=\int_{-\infty}^{\infty}f(\tau)g(t-\tau)\ (连续形式) \\
(f*g)=\sum_{\tau=-\infty}^{\infty}f(\tau)g(t-\tau)\ (离散形式)
$$


#### 实例

想象我们现在有两个骰子，两个骰子分别是$f$跟$g$，$f(1)$表示骰子$f$向上一面为数字1的概率。同时抛掷这两个骰子1次，它们正面朝上数字和为4的概率是多少呢？相信大家很快就能想出它包含了三种情况，分别是：

$f$向上为1，$g$向上为3；

$f$向上为2，$g$向上为2；

$f$向上为3，$g$向上为1；

最后这三种情况出现的概率和即问题的答案，如果写成公式，就是 $\sum_{\tau=1}^3f(\tau)g(4-\tau)$。可以形象的绘制成下图：

![筛子](https://raw.githubusercontent.com/SivilTaram/Graph-Neural-Network-Note/master/images/image-11-convolution-basic.png)

如果稍微扩展一点，比如说我们认为 $f(0)$或者 $g(0)$ 等是可以取到的，只是它们的值为0而已。那么该公式可以写成$\sum_{\tau=-\infty}^{\infty}f(\tau)g(4-\tau)$。仔细观察，这其实就是卷积$(f*g)(4)$。如果将它写成内积的形式，卷积其实就是 $[f(-\infty),...，f(1),...,f(\infty)]\cdot[g(\infty),...,g(3),...g(-\infty)]$。这么一看，是不是就对卷积的名字理解更深刻了呢? 所谓卷积，其实就是把一个函数卷(翻)过来，然后与另一个函数求内积。

对应到不同方面，卷积可以有不同的解释：$g$既可以看作我们在深度学习里常说的**核**(Kernel)，也可以对应到信号处理中的**滤波器**(Filter)。而$f$可以是我们所说的机器学习中的**特征**(Feature)，也可以是信号处理中的**信号**(Signal)。$f$和$g$的卷积$(f*g)$就可以看作是对$f$的加权求和。下面两个动图就分别对应信号处理与深度学习中卷积操作的过程。



![信号处理图示](https://raw.githubusercontent.com/SivilTaram/Graph-Neural-Network-Note/master/images/image-12-conv-signal.gif)

![深度学习](https://raw.githubusercontent.com/SivilTaram/Graph-Neural-Network-Note/master/images/image-13-conv-cnn.gif)

